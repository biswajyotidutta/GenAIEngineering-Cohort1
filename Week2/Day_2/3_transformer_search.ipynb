{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6650891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98e0832",
   "metadata": {},
   "source": [
    "# NLP Environment Setup for Semantic Analysis\n",
    "\n",
    "This code sets up a comprehensive environment for natural language processing with a focus on semantic analysis:\n",
    "\n",
    "### Core Components\n",
    "\n",
    "1. **System and Environment Tools**\n",
    "   - `os`: Operating system interfaces for file operations\n",
    "   - `json`: JSON data handling for structured data exchange\n",
    "   - `datetime`: Date and time manipulation\n",
    "   - `dotenv`: Environment variable management from .env files\n",
    "\n",
    "2. **Hugging Face Integration**\n",
    "   - `HfApi`: Interface to interact with the Hugging Face model hub\n",
    "   - `hf_hub_download`: Direct download functionality for models and datasets\n",
    "   - `pipeline`: High-level API for using pre-trained models\n",
    "\n",
    "3. **Data Visualization**\n",
    "   - `numpy`: Numerical computing foundation\n",
    "   - `matplotlib.pyplot`: Core plotting capabilities\n",
    "   - `seaborn`: Enhanced statistical data visualization\n",
    "\n",
    "4. **Semantic Text Analysis**\n",
    "   - `SentenceTransformer`: Specialized library for generating text embeddings\n",
    "     - Creates vector representations that capture semantic meaning\n",
    "     - Enables similarity comparisons, clustering, and semantic search\n",
    "\n",
    "This setup provides all the necessary tools for tasks like:\n",
    "- Downloading and using pre-trained NLP models\n",
    "- Computing semantic similarity between texts\n",
    "- Visualizing relationships between text embeddings\n",
    "- Creating embeddings for search or recommendation systems\n",
    "\n",
    "The combination of Hugging Face's model ecosystem with SentenceTransformer's specialized embedding capabilities offers a powerful foundation for advanced text analysis applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3de16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, datetime\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4820b3",
   "metadata": {},
   "source": [
    "\n",
    "# Loading Environment Variables for Hugging Face\n",
    "\n",
    "\n",
    "This code snippet performs two essential operations:\n",
    "\n",
    "1. `load_dotenv()` - Loads environment variables from a `.env` file into the application's environment. This is a common pattern for securely storing configuration and sensitive information outside of the source code.\n",
    "\n",
    "2. `hf_key = os.getenv(\"HF_TOKEN\")` - Retrieves the Hugging Face API token from the environment variables and assigns it to the variable `hf_key`. This token is required for authenticated access to the Hugging Face Hub services, including downloading private models or models with gated access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_key=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f29e96",
   "metadata": {},
   "source": [
    "# Setting Up Sentence Transformer for Text Embeddings\n",
    "\n",
    "# Hugging Face Model Reference\n",
    "\n",
    "[sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)\n",
    "\n",
    "## Model Details\n",
    "\n",
    "This code initializes a state-of-the-art sentence embedding model from the Sentence Transformers library:\n",
    "\n",
    "### Model Selection\n",
    "- **Model ID**: `all-mpnet-base-v2`\n",
    "- **Architecture**: MPNet (Masked and Permuted Pre-training for Language Understanding)\n",
    "- **Performance**: One of the best-performing general-purpose embedding models\n",
    "- **Vector Size**: 768 dimensions\n",
    "\n",
    "### Key Capabilities\n",
    "- Creates dense vector representations that capture semantic meaning\n",
    "- Consistently ranks among top models on semantic textual similarity benchmarks\n",
    "- Performs well on a wide range of tasks without task-specific fine-tuning\n",
    "- Balances quality and efficiency better than larger models\n",
    "\n",
    "### Technical Specifications\n",
    "- Built on Microsoft's MPNet architecture\n",
    "- Fine-tuned on over 1 billion training pairs\n",
    "- Optimized for semantic similarity tasks\n",
    "- Works with sentences, paragraphs, and short documents\n",
    "\n",
    "This model is particularly effective for:\n",
    "- Semantic search\n",
    "- Clustering similar texts\n",
    "- Finding document similarities\n",
    "- Text classification\n",
    "- Information retrieval\n",
    "\n",
    "The model will be downloaded on first use and cached locally for future access, making subsequent operations much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f49c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_reference='sentence-transformers/all-mpnet-base-v2'\n",
    "hf_model_cache=SentenceTransformer(hf_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8bd298",
   "metadata": {},
   "source": [
    "# Dataset for Semantic Text Analysis\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "This code creates a curated dataset for demonstrating semantic similarity and clustering:\n",
    "\n",
    "### Content Organization\n",
    "- **10 sentences** grouped into 3 distinct topic categories\n",
    "- Balanced representation with varying numbers of examples per category\n",
    "\n",
    "### Topic Distribution\n",
    "1. **Technology (4 sentences)**\n",
    "   - Covers AI, smartphones, cloud computing, and IoT\n",
    "   - Connected by themes of digital technology and innovation\n",
    "\n",
    "2. **Nature (3 sentences)**\n",
    "   - Includes geological features, ecosystems, and climate\n",
    "   - Connected by themes of natural environments and systems\n",
    "\n",
    "3. **Food (3 sentences)**\n",
    "   - Represents cuisine, flavoring, and dietary choices\n",
    "   - Connected by themes of culinary concepts and eating\n",
    "\n",
    "### Labels Array\n",
    "- Parallel array with categorical labels matching each sentence\n",
    "- Provides ground truth for evaluating clustering and classification tasks\n",
    "- Enables color-coding in visualizations\n",
    "\n",
    "This structured dataset is ideal for demonstrating how embedding models like MPNet can automatically discover semantic relationships between texts, even when they contain no overlapping keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 10 sentences across 3 subjects\n",
    "sentences = [\n",
    "    # Technology (4 sentences)\n",
    "    \"Artificial intelligence is reshaping our future.\",\n",
    "    \"Smartphones have revolutionized modern communication.\",\n",
    "    \"Cloud computing enables remote data storage and access.\",\n",
    "    \"The internet of things connects everyday devices online.\",\n",
    "    \n",
    "    # Nature (3 sentences)\n",
    "    \"Mountains are formed by tectonic plate movements.\",\n",
    "    \"Rainforests provide habitat for countless species.\",\n",
    "    \"Oceans regulate the global climate system.\",\n",
    "    \n",
    "    # Food (3 sentences)\n",
    "    \"Italian cuisine is famous for pasta and pizza.\",\n",
    "    \"Spices enhance the flavor of many dishes.\",\n",
    "    \"Vegetarian diets exclude meat and fish.\"\n",
    "]\n",
    "\n",
    "# Create labels for each sentence to identify the subject\n",
    "labels = [\"Tech\", \"Tech\", \"Tech\", \"Tech\", \"Nature\", \"Nature\", \"Nature\", \"Food\", \"Food\", \"Food\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b7bf65",
   "metadata": {},
   "source": [
    "# Understanding Sentence Embeddings\n",
    "\n",
    "## Output Analysis\n",
    "\n",
    "This code generates and displays the vector embedding for the first sentence in our dataset:\n",
    "\n",
    "### Process:\n",
    "- `hf_model_cache.encode()`: Passes the sentence through the MPNet model\n",
    "- `sentences[0]`: \"Artificial intelligence is reshaping our future.\"\n",
    "- `convert_to_numpy=True`: Returns the embedding as a NumPy array\n",
    "\n",
    "### Embedding Dimensions:\n",
    "- `embeddings.shape`: Will show `(768,)`\n",
    "- This is a one-dimensional vector with 768 elements\n",
    "- Each element (or feature) captures different semantic aspects of the text\n",
    "\n",
    "### Vector Content:\n",
    "- The output displays the actual numeric values in the embedding\n",
    "- Values typically range between -1 and 1\n",
    "- These numbers represent the sentence in a high-dimensional semantic space\n",
    "- Similar sentences will have similar patterns of values\n",
    "\n",
    "### Significance:\n",
    "This dense vector representation captures the semantic meaning of the sentence in a way that computers can process mathematically. The 768 dimensions collectively encode information about:\n",
    "\n",
    "- Topic (technology, AI)\n",
    "- Sentiment (neutral/positive discussion of future impact)\n",
    "- Context (technological development and societal change)\n",
    "\n",
    "The embedding enables algorithms to measure similarity between texts, even when they use different words to express related concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0bf4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=hf_model_cache.encode(sentences[0], convert_to_numpy=True)\n",
    "print(embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cba1bd",
   "metadata": {},
   "source": [
    "# Processing Multiple Sentences with Sentence Transformer\n",
    "\n",
    "## Output Analysis\n",
    "\n",
    "This code generates vector embeddings for all sentences in our dataset simultaneously:\n",
    "\n",
    "### Process:\n",
    "- `hf_model_cache.encode()`: Processes multiple sentences in a batch\n",
    "- `sentences`: The full list of 10 sentences across three topics\n",
    "- `convert_to_numpy=True`: Returns results as a NumPy array\n",
    "\n",
    "### Embedding Dimensions:\n",
    "- `embeddings.shape`: Will show `(10, 768)`\n",
    "- This is a two-dimensional array (matrix):\n",
    "  - First dimension (10): Number of sentences\n",
    "  - Second dimension (768): Embedding size for each sentence\n",
    "\n",
    "### Efficiency Benefits:\n",
    "- **Batch Processing**: Much faster than encoding sentences one by one\n",
    "- **Vectorization**: Leverages GPU acceleration when available\n",
    "- **Memory Efficiency**: Processes all sentences in a single operation\n",
    "\n",
    "### Technical Details:\n",
    "- Each row in the matrix contains the 768-dimensional embedding for one sentence\n",
    "- All sentences are represented in the same semantic space\n",
    "- The matrix structure enables easy computation of similarity metrics:\n",
    "  - Cosine similarity\n",
    "  - Euclidean distance\n",
    "  - Dot products\n",
    "\n",
    "This matrix of embeddings provides the foundation for analyzing semantic relationships between all sentences in our dataset, enabling clustering, visualization, and similarity detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb000df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = hf_model_cache.encode(sentences, convert_to_numpy=True)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c518bcb",
   "metadata": {},
   "source": [
    "# Visualizing Semantic Embeddings with a Heatmap\n",
    "\n",
    "## Process Explanation\n",
    "\n",
    "This code creates a visual representation of the sentence embeddings as a heatmap:\n",
    "\n",
    "### Data Preparation\n",
    "- **Dimension Reduction**: `dimensions=100` selects only the first 100 dimensions (out of 768)\n",
    "  - Makes visualization more manageable\n",
    "  - Focuses on the dimensions with higher variance\n",
    "  - Reduces computational load\n",
    "\n",
    "- **Value Normalization**: Scales all values to the range [0,1]\n",
    "  - `min_val` and `max_val`: Find the global minimum and maximum\n",
    "  - Formula: `(value - min) / (max - min)`\n",
    "  - Creates consistent color mapping across all values\n",
    "\n",
    "### Visualization Configuration\n",
    "- **Figure Size**: 16×10 inches provides ample space for the detailed matrix\n",
    "- **Color Scheme**: \"coolwarm\" uses blue for negative values and red for positive\n",
    "- **Center Point**: Sets white as the neutral/zero point\n",
    "\n",
    "### Semantic Context\n",
    "- **Y-axis Labels**: Combines category labels with sentence text\n",
    "  - Format: \"Tech: Artificial intelligence is reshaping our future.\"\n",
    "  - Enables quick identification of patterns by category\n",
    "\n",
    "- **X-axis Labels**: Identifies each dimension\n",
    "  - Format: \"dim_0\", \"dim_1\", etc.\n",
    "  - Provides reference for specific embedding components\n",
    "\n",
    "### Visual Patterns\n",
    "In the resulting heatmap:\n",
    "- Sentences from the same category (Tech, Nature, Food) will show similar color patterns\n",
    "- Distinct color profiles emerge for different semantic categories\n",
    "- The visual similarities directly reflect semantic relationships\n",
    "- Some dimensions will appear more active (darker colors) for specific topics\n",
    "\n",
    "This visualization helps identify which embedding dimensions capture particular semantic concepts and how sentences cluster based on their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077049d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions=100\n",
    "embedding_subset = embeddings[:, :dimensions]\n",
    "min_val = np.min(embedding_subset)\n",
    "max_val = np.max(embedding_subset)\n",
    "normalized_embeddings = (embedding_subset - min_val) / (max_val - min_val)\n",
    "\n",
    "heatmap_embeddings = embedding_subset\n",
    "# heatmap_embeddings = normalized_embeddings\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "ax = sns.heatmap(\n",
    "    normalized_embeddings, \n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    yticklabels=[f\"{label}: {sent}\" for label, sent in zip(labels, sentences)],\n",
    "    xticklabels=[f\"dim_{i}\" for i in range(len(normalized_embeddings[0]))]  # Label each dimension\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
