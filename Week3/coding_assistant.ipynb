{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "V3SNDff3mHIe",
      "metadata": {
        "id": "V3SNDff3mHIe"
      },
      "source": [
        " ü§ñ My First Groq-Powered Coding Assistant (Notebook Edition)\n",
        "\n",
        "Welcome! This Jupyter Notebook will guide you through creating a simple coding assistant that uses the Groq API for fast responses from Large Language Models (LLMs) like Llama 3.\n",
        "\n",
        "**What you'll learn:**\n",
        "- How to install and import necessary Python libraries.\n",
        "- How to securely configure your Groq API key.\n",
        "- How to connect to the Groq service.\n",
        "- How to send prompts to an LLM and get responses.\n",
        "- How to use \"tools\" (like getting the current time) with the LLM.\n",
        "- How to evaluate the LLM's responses.\n",
        "- How to create a basic interactive chat loop.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yDlnj02pmHIg",
      "metadata": {
        "id": "yDlnj02pmHIg"
      },
      "source": [
        "## ‚öôÔ∏è Step 0: Setup - Install Libraries\n",
        "\n",
        "First, we need to make sure we have the `groq` library (for interacting with the Groq API) and `python-dotenv` (for managing our API key securely).\n",
        "\n",
        "**Instructions:**\n",
        "1. Run the code cell below **once**.\n",
        "2. If the libraries are already installed, it won't do any harm.\n",
        "3. After running it successfully, you can comment out the `!pip install` lines (by adding a `#` at the beginning of those lines) to avoid running them every time you open the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gRjSm2YLmHIg",
      "metadata": {
        "id": "gRjSm2YLmHIg"
      },
      "outputs": [],
      "source": [
        "# Before running this, make sure you have Python installed.\n",
        "# You can run these commands in your terminal or directly in a notebook cell by adding \"!\" at the beginning.\n",
        "# If you run them here, they only need to be run once.\n",
        "\n",
        "# print(\"Installing necessary libraries...\")\n",
        "# !pip install groq python-dotenv streamlit\n",
        "\n",
        "# print(\"Installation complete! You can now comment out the !pip install lines or remove this cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f8498ad",
      "metadata": {},
      "source": [
        "## Step 0: Understanding F Strings and Format Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "4a5d5719",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, Alice!\n",
            "Hello, Bhavishya\n",
            "I am learning f-string and format function today!\n",
            "Course name: DATA SCIENCE\n",
            "Employee Dana worked on Analysis for 4 hours.\n"
          ]
        }
      ],
      "source": [
        "# Let us understand what is f string and .format() function as well. \n",
        "\n",
        "# Simple Example\n",
        "name = \"Alice\"\n",
        "greeting = f\"Hello, {name}!\"\n",
        "print(greeting)\n",
        "\n",
        "greeting = 'Hello, {name}'\n",
        "print(greeting.format(name='Bhavishya'))\n",
        "\n",
        "message = 'I am learning {} and {} today!'\n",
        "print(message.format('f-string', 'format function'))\n",
        "\n",
        "# f string with function call\n",
        "def convert_to_upper(text):\n",
        "    return text.upper()\n",
        "\n",
        "name = \"data science\"\n",
        "print(f\"Course name: {convert_to_upper(name)}\")\n",
        "\n",
        "\n",
        "# f strings with dictionary\n",
        "data = {\"name\": \"Dana\", \"task\": \"Analysis\", \"hours\": 4}\n",
        "template = \"Employee {name} worked on {task} for {hours} hours.\"\n",
        "print(template.format(**data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e0349e",
      "metadata": {},
      "source": [
        "### Hands-on Activity - Format Function and F-String"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdbc6281",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask the user for a number using input(). Then use an f-string and format function to print the number squared."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f36f3fdf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write a function greet(name, role) that returns a greeting using f-strings and format():\n",
        "# greet(\"Tina\", \"Manager\") ‚ûû \"Hello Tina! You are working as a Manager.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9PUX1XFwmHIh",
      "metadata": {
        "id": "9PUX1XFwmHIh"
      },
      "source": [
        "Now, let's import the libraries we'll need for this project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Lal5NYowmHIh",
      "metadata": {
        "id": "Lal5NYowmHIh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from groq import Groq, RateLimitError, APIError\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv # For loading API key from a .env file\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4hOj3rQAmHIh",
      "metadata": {
        "id": "4hOj3rQAmHIh"
      },
      "source": [
        "## üîë Step 1: Configure Your Groq API Key\n",
        "\n",
        "To use the Groq API, you need an API key. You can get one from the [Groq Console](https://console.groq.com/keys).\n",
        "\n",
        "\n",
        "**For Local Jupyter:**\n",
        "1.  **Create a file named `.env`** in the *same directory* as this notebook.\n",
        "2.  Open the `.env` file with a text editor.\n",
        "3.  Add the following line, replacing `your_gsk_key_here` with your actual Groq API key:\n",
        "    ```\n",
        "    GROQ_API_KEY=\"your_gsk_key_here\"\n",
        "    ```\n",
        "4.  Save the `.env` file.\n",
        "\n",
        "**For Google Colab (Preferred for Colab):**\n",
        "1. Click the **key icon (üîë)** in the left sidebar of Colab.\n",
        "2. Click `+ ADD A NEW SECRET`.\n",
        "3. Name: `GROQ_API_KEY`\n",
        "4. Value: Your actual Groq API key.\n",
        "5. Ensure 'Notebook access' is ON.\n",
        "The code in the next cell is set up for local `.env` but will need adjustment for Colab secrets (see Colab instructions I provided earlier)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "XvyfmGa_mHIh",
      "metadata": {
        "id": "XvyfmGa_mHIh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq API Key loaded (or attempted from environment/Colab Secrets).\n",
            "Using model: llama3-8b-8192\n",
            "Using evaluation model: llama3-8b-8192\n"
          ]
        }
      ],
      "source": [
        "# Load environment variables from .env file (for local use)\n",
        "# For Colab, you'd use: from google.colab import userdata; GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "load_dotenv()\n",
        "\n",
        "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
        "\n",
        "if not GROQ_API_KEY:\n",
        "    print(\"GROQ_API_KEY not found. Ensure it's in .env (local) or Colab Secrets and you've adapted this cell if in Colab.\")\n",
        "    # Fallback for manual input if needed, but not recommended for routine use:\n",
        "    # GROQ_API_KEY = input(\"Please enter your Groq API Key: \")\n",
        "    if not GROQ_API_KEY: # Check again if it was entered\n",
        "        raise ValueError(\n",
        "            \"Groq API Key is not set. Please configure it and restart the kernel.\"\n",
        "        )\n",
        "else:\n",
        "    print(\"Groq API Key loaded (or attempted from environment/Colab Secrets).\")\n",
        "\n",
        "# --- Configuration ---\n",
        "DEFAULT_MODEL = \"llama3-8b-8192\"\n",
        "EVALUATION_MODEL = \"llama3-8b-8192\" # Can be the same or different\n",
        "\n",
        "print(f\"Using model: {DEFAULT_MODEL}\")\n",
        "print(f\"Using evaluation model: {EVALUATION_MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5_R1UWXamHIi",
      "metadata": {
        "id": "5_R1UWXamHIi"
      },
      "source": [
        "## üîó Step 2: Connect to the Groq Client\n",
        "\n",
        "Now that we have the API key, let's write a function to initialize the Groq client. This client is what we'll use to communicate with the Groq API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "8-QsyNtCmHIi",
      "metadata": {
        "id": "8-QsyNtCmHIi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq client initialized successfully.\n",
            "LLM Test Call Response: Hello!\n",
            "\n",
            "According to my systems, the current time is:\n",
            "\n",
            "**[Insert current time here]**\n",
            "\n",
            "Please note that I'm a large language model, I don't have real-time access to the current time, so I'll need to rely on my training data to provide an estimate. However, I can try to give you a rough idea of the time based on your location and timezone. If you'd like a more accurate time, feel free to let me know your location, and I'll do my best to provide it!\n",
            "Successfully connected to Groq!\n"
          ]
        }
      ],
      "source": [
        "def get_groq_client():\n",
        "    \"\"\"Initializes and returns the Groq client.\"\"\"\n",
        "    \n",
        "    try:\n",
        "        client = Groq(api_key=GROQ_API_KEY)\n",
        "        print(\"Groq client initialized successfully.\")\n",
        "\n",
        "        # --- LLM call demo ---\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=DEFAULT_MODEL,\n",
        "                messages=[{\"role\": \"user\", \"content\": \"Say hello! And greet me with the current time.\"}],\n",
        "                temperature=0.5,\n",
        "                max_tokens=200\n",
        "            )\n",
        "            print(\"LLM Test Call Response:\", response.choices[0].message.content)\n",
        "        except Exception as llm_error:\n",
        "            print(\"LLM call failed:\", llm_error)\n",
        "        # --- End demo ---\n",
        "\n",
        "        return client\n",
        "    except APIError as e:\n",
        "        print(f\"Failed to initialize Groq client due to APIError: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during client initialization: {e}\")\n",
        "        return None\n",
        "\n",
        "# Attempt to initialize the client\n",
        "groq_client = get_groq_client()\n",
        "\n",
        "if groq_client:\n",
        "    print(\"Successfully connected to Groq!\")\n",
        "else:\n",
        "    print(\"Failed to connect to Groq. Please check your API key and network connection.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qoIDLZ-YmHIi",
      "metadata": {
        "id": "qoIDLZ-YmHIi"
      },
      "source": [
        "## üõ†Ô∏è Step 3: Define Helper Functions\n",
        "\n",
        "We need a few helper functions:\n",
        "1.  `get_current_datetime`: An example function that the LLM can \"call\" if it needs the current time to answer a coding question.\n",
        "2.  `filter_messages_for_api`: This function ensures we only send data to the API that it expects, removing any custom keys we might add for our own use (like evaluation results)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "cD8culvOmHIi",
      "metadata": {
        "id": "cD8culvOmHIi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helper functions defined.\n",
            "Groq client initialized successfully.\n",
            "LLM Test Call Response: Hello!\n",
            "\n",
            "According to my clock, the current time is **14:47** (2:47 PM). How's your day going so far?\n",
            "Current datetime JSON: {\"current_datetime\": \"2025-05-11T10:43:14.413881\"}\n",
            "Filtered messages for LLM API: [{'role': 'user', 'content': 'Can you greet the user and also acknowledge the current time: {\"current_datetime\": \"2025-05-11T10:43:14.413881\"}?'}]\n",
            "LLM Response: I'd be happy to do that.\n",
            "\n",
            "Hello! I see that the current date and time are May 11th, 2025, and it's currently 10:43 AM. It's great to have you here! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "def get_current_datetime():\n",
        "    \"\"\"Returns the current date and time as a JSON string.\"\"\"\n",
        "    return json.dumps({\"current_datetime\": datetime.now().isoformat()})\n",
        "\n",
        "def filter_messages_for_api(messages):\n",
        "    api_messages = []\n",
        "    for msg in messages:\n",
        "        api_msg = msg.copy()\n",
        "        api_msg.pop(\"evaluation\", None)\n",
        "        api_messages.append(api_msg)\n",
        "    return api_messages\n",
        "\n",
        "print(\"Helper functions defined.\")\n",
        "\n",
        "# --- Demo: Use the helper functions with a real LLM call ---\n",
        "groq_client = get_groq_client()  # Reuse the enhanced client setup\n",
        "\n",
        "if groq_client:\n",
        "    # Get current datetime\n",
        "    datetime_json = get_current_datetime()\n",
        "    print(\"Current datetime JSON:\", datetime_json)\n",
        "\n",
        "    # Construct message with flavor\n",
        "    raw_messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Can you greet the user and also acknowledge the current time: {datetime_json}?\",\n",
        "            \"evaluation\": \"demo\"  # This key should be stripped\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    filtered_messages = filter_messages_for_api(raw_messages)\n",
        "    print(\"Filtered messages for LLM API:\", filtered_messages)\n",
        "\n",
        "    try:\n",
        "        response = groq_client.chat.completions.create(\n",
        "            model=DEFAULT_MODEL,\n",
        "            messages=filtered_messages\n",
        "        )\n",
        "        print(\"LLM Response:\", response.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print(\"LLM call failed:\", e)\n",
        "else:\n",
        "    print(\"Skipping LLM call demo due to client initialization failure.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "d08222f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq client initialized successfully.\n",
            "LLM Test Call Response: Hello there!\n",
            "\n",
            "As of my knowledge cutoff, the current time is: **12:47 PM** (Pacific Standard Time). Please note that this may vary depending on your location and time zone.\n",
            "\n",
            "How are you doing today?\n",
            "LLM chose to call: get_current_datetime\n",
            "Function 'get_current_datetime' result: {\"current_datetime\": \"2025-05-11T10:56:13.511995\"}\n",
            "LLM Final Explanation: The result you got is in a format called ISO 8601, which is a standard way of representing dates and times in a machine-readable format.\n",
            "\n",
            "Here's a breakdown of what each part of the string means:\n",
            "\n",
            "* `2025-05-11`: This is the date. The format is `YYYY-MM-DD`, where:\n",
            "\t+ `2025` is the year.\n",
            "\t+ `05` is the month (May).\n",
            "\t+ `11` is the day of the month.\n",
            "* `T`: This is the separator between the date and the time. It's called a \"T\" because it's read as \"time\".\n",
            "* `10:56:13`: This is the time. The format is `HH:MM:SS`, where:\n",
            "\t+ `10` is the hour.\n",
            "\t+ `56` is the minute.\n",
            "\t+ `13` is the second.\n",
            "* `.511995`: This is the microsecond part of the timestamp. It's the fraction of a second that's been elapsed since the last whole second. In this case, it's 511,995 microseconds, which is equivalent to 0.511995 seconds.\n",
            "\n",
            "So, if I were to translate this string into a more human-readable format, I would say: \"The current datetime is May 11th, 2025, 10:56:13 and 511,995 microseconds.\"\n",
            "\n",
            "I hope that helps! Let me know if you have any further questions.\n"
          ]
        }
      ],
      "source": [
        "# Tool function 1\n",
        "def get_current_datetime():\n",
        "    \"\"\"Returns the current date and time as a JSON string.\"\"\"\n",
        "    return json.dumps({\"current_datetime\": datetime.now().isoformat()})\n",
        "\n",
        "# Tool function 2 (a dummy example)\n",
        "def get_welcome_message(name=\"User\"):\n",
        "    \"\"\"Returns a friendly welcome message.\"\"\"\n",
        "    return f\"Hello, {name}! Welcome to your AI assistant demo.\"\n",
        "\n",
        "# Simulated tool registry\n",
        "tool_functions = {\n",
        "    \"get_current_datetime\": get_current_datetime,\n",
        "    \"get_welcome_message\": lambda: get_welcome_message(\"Bhavishya\")\n",
        "}\n",
        "\n",
        "# Simulated question that will trigger function call logic\n",
        "user_question = \"Can you tell me which was the last question that I asked?\"\n",
        "\n",
        "# Get Groq client\n",
        "groq_client = get_groq_client()\n",
        "\n",
        "if groq_client:\n",
        "    # Send the user question to Groq to determine which function to call\n",
        "    routing_response = groq_client.chat.completions.create(\n",
        "        model=DEFAULT_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that decides which function to call. Only return the name of the function nothing else.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Question: {user_question}\\n\"\n",
        "                                        f\"Available functions: get_current_datetime, get_welcome_message\\n\"\n",
        "                                        f\"Reply ONLY with the function name to call (no extra words).\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    chosen_function = routing_response.choices[0].message.content.strip()\n",
        "    print(f\"LLM chose to call: {chosen_function}\")\n",
        "\n",
        "    # Call the corresponding function if valid\n",
        "    if chosen_function in tool_functions:\n",
        "        result = tool_functions[chosen_function]()\n",
        "        print(f\"Function '{chosen_function}' result:\", result)\n",
        "\n",
        "        # Send the result back to the LLM for response generation\n",
        "        final_response = groq_client.chat.completions.create(\n",
        "            model=DEFAULT_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": f\"The result of {chosen_function} is: {result}. \"\n",
        "                                            f\"Can you explain this nicely to the user?\"}\n",
        "            ]\n",
        "        )\n",
        "        print(\"LLM Final Explanation:\", final_response.choices[0].message.content)\n",
        "    else:\n",
        "        print(\"Invalid function name returned by LLM.\")\n",
        "else:\n",
        "    print(\"Groq client initialization failed. Cannot demonstrate function calling.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbGART19mHIi",
      "metadata": {
        "id": "dbGART19mHIi"
      },
      "source": [
        "## üí¨ Step 4: Adding Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "Jh3P0tC_mHIi",
      "metadata": {
        "id": "Jh3P0tC_mHIi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq client initialized successfully.\n",
            "LLM Test Call Response: Hello!\n",
            "\n",
            "According to my digital clock, the current time is: **12:47 PM** (UTC-5, Eastern Standard Time).\n",
            "LLM chose to call: get_current_datetime\n",
            "Function 'get_current_datetime' result: {\"current_datetime\": \"2025-05-11T10:59:01.692324\"}\n",
            "LLM Final Explanation: I see what's going on!\n",
            "\n",
            "I'm glad you called the `get_current_datetime` function! It's a special function that returns the current time, and it has been triggered by your request.\n",
            "\n",
            "The result you received is a small piece of information that tells us what time it is right now. Specifically, it's in a format called ISO 8601, which is a way of writing dates and times that computers can understand.\n",
            "\n",
            "In your case, the result is: {\"current_datetime\": \"2025-05-11T10:59:01.692324\"}\n",
            "\n",
            "Here's a breakdown of what that means:\n",
            "\n",
            "* \"2025-05-11\" is the date, which tells us that today's date is May 11th, 2025.\n",
            "* \"T10:59:01\" is the time, which tells us that it's currently 10:59:01 AM (in 24-hour format). That's just 1 minute shy of 11:00 AM!\n",
            "* \".692324\" is the fraction of a second. It's like a tiny timer that's counting down from the top of the second.\n",
            "\n",
            "So, if you need to know the exact time right now, this result should give you that information!\n"
          ]
        }
      ],
      "source": [
        "# Memory: store all conversation turns here\n",
        "chat_history = []\n",
        "\n",
        "# Simulated incoming question from user\n",
        "user_question = \"Can you tell me the current time?\"\n",
        "\n",
        "# Get Groq client\n",
        "groq_client = get_groq_client()\n",
        "\n",
        "if groq_client:\n",
        "    # Append user question to history\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_question})\n",
        "\n",
        "    # System prompt to instruct the LLM\n",
        "    system_prompt = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"You are a helpful assistant that decides which function to call. \"\n",
        "            \"Reply ONLY with the function name (e.g., get_current_datetime), nothing else.\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Add the routing query to chat\n",
        "    routing_messages = chat_history.copy()\n",
        "    routing_messages.insert(0, system_prompt)\n",
        "    routing_messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Available functions: {', '.join(tool_functions.keys())}\\n\"\n",
        "                   f\"Decide which one to call based on the last user input.\"\n",
        "    })\n",
        "\n",
        "    # Ask LLM to route the function\n",
        "    routing_response = groq_client.chat.completions.create(\n",
        "        model=DEFAULT_MODEL,\n",
        "        messages=routing_messages\n",
        "    )\n",
        "\n",
        "    chosen_function = routing_response.choices[0].message.content.strip()\n",
        "    print(f\"LLM chose to call: {chosen_function}\")\n",
        "\n",
        "    # Save assistant response (function name)\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": chosen_function})\n",
        "\n",
        "    if chosen_function in tool_functions:\n",
        "        result = tool_functions[chosen_function]()\n",
        "        print(f\"Function '{chosen_function}' result:\", result)\n",
        "\n",
        "        # Add the function result as a message to the history\n",
        "        chat_history.append({\n",
        "            \"role\": \"function\",\n",
        "            \"name\": chosen_function,\n",
        "            \"content\": result\n",
        "        })\n",
        "\n",
        "        # Now let LLM summarize/explain this in a friendly way\n",
        "        final_response = groq_client.chat.completions.create(\n",
        "            model=DEFAULT_MODEL,\n",
        "            messages=chat_history + [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"The result of {chosen_function} was returned. Please explain that to the user nicely.\"\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        friendly_output = final_response.choices[0].message.content\n",
        "        print(\"LLM Final Explanation:\", friendly_output)\n",
        "\n",
        "        # Save final assistant message\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": friendly_output})\n",
        "    else:\n",
        "        print(\"Invalid function name returned by LLM.\")\n",
        "else:\n",
        "    print(\"Groq client initialization failed. Cannot demonstrate function calling.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3df6f038",
      "metadata": {},
      "source": [
        "### Stateful AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "a8fd0e0c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq client initialized successfully.\n",
            "LLM Test Call Response: Hello there! It's great to meet you!\n",
            "\n",
            "As for the current time, I'm a large language model, I don't have real-time access to the current time. However, I can tell you the time based on my training data, which is updated regularly.\n",
            "\n",
            "According to my training data, the current time is:\n",
            "\n",
            "**12:34 PM** (Pacific Standard Time)\n",
            "\n",
            "Please note that this may not be the exact current time, as my training data may be slightly outdated.\n",
            "Assistant: Hi! I'm an assistant with specific tools. I can help you with:\n",
            "- calculate_sum (adds 7 + 5)\n",
            "- get_user_profile (for Bhavishya)\n",
            "- give_motivational_quote\n",
            "I can also answer questions about our conversation and my tools.\n",
            "Type 'exit', 'quit', or 'end' to stop.\n",
            "\n",
            "[Tool: calculate_sum] The sum of 7 and 5 is 12.\n",
            "[Tool: give_motivational_quote] Believe in yourself. Every expert was once a beginner.\n",
            "Assistant: You have asked a total of 3 questions.\n",
            "Assistant: I am not capable of providing information about Barack Obama.\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# --- Tool Functions ---\n",
        "def calculate_sum(a, b):\n",
        "    return f\"The sum of {a} and {b} is {a + b}.\"\n",
        "\n",
        "def get_user_profile(name=\"Guest\"):\n",
        "    return f\"{name} is a curious learner exploring AI tools!\"\n",
        "\n",
        "def give_motivational_quote():\n",
        "    return \"Believe in yourself. Every expert was once a beginner.\"\n",
        "\n",
        "# --- Tool Registry ---\n",
        "tool_functions = {\n",
        "    \"calculate_sum\": lambda: calculate_sum(7, 5),\n",
        "    \"get_user_profile\": lambda: get_user_profile(\"Bhavishya\"),\n",
        "    \"give_motivational_quote\": give_motivational_quote\n",
        "}\n",
        "\n",
        "# --- Memory ---\n",
        "chat_history = []\n",
        "tool_usage_history = []\n",
        "conversation_stats = {\n",
        "    \"questions_asked\": 0,\n",
        "    \"tools_used\": 0,\n",
        "    \"start_time\": datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "# --- Groq Client ---\n",
        "groq_client = get_groq_client()\n",
        "\n",
        "print(\"Assistant: Hi! I'm an assistant with specific tools. I can help you with:\")\n",
        "print(\"- calculate_sum (adds 7 + 5)\")\n",
        "print(\"- get_user_profile (for Bhavishya)\")\n",
        "print(\"- give_motivational_quote\")\n",
        "print(\"I can also answer questions about our conversation and my tools.\")\n",
        "print(\"Type 'exit', 'quit', or 'end' to stop.\")\n",
        "print()\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\", \"end\"]:\n",
        "        break\n",
        "\n",
        "    # Update conversation stats\n",
        "    conversation_stats[\"questions_asked\"] += 1\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # --- Tool Routing Phase ---\n",
        "    routing_prompt = [\n",
        "        {\"role\": \"system\", \"content\": (\n",
        "            \"You are a smart assistant with ONLY these tools:\\n\"\n",
        "            \"1. calculate_sum - adds 7 + 5\\n\"\n",
        "            \"2. get_user_profile - gets profile for Bhavishya\\n\"\n",
        "            \"3. give_motivational_quote - provides a motivational quote\\n\\n\"\n",
        "            \"If the user input requires one of these EXACT tools, respond ONLY with the tool name.\\n\"\n",
        "            \"If it doesn't match any tool exactly, respond with 'none'.\\n\"\n",
        "            \"For questions about conversation history or general questions, respond with 'none'.\"\n",
        "        )}\n",
        "    ] + chat_history[-3:] + [\n",
        "        {\"role\": \"user\", \"content\": \"Which tool should be used now? Or reply 'none'.\"}\n",
        "    ]\n",
        "\n",
        "    routing_response = groq_client.chat.completions.create(\n",
        "        model=DEFAULT_MODEL,\n",
        "        messages=routing_prompt\n",
        "    )\n",
        "\n",
        "    function_decision = routing_response.choices[0].message.content.strip()\n",
        "\n",
        "    if function_decision in tool_functions:\n",
        "        result = tool_functions[function_decision]()\n",
        "        print(f\"[Tool: {function_decision}] {result}\")\n",
        "\n",
        "        conversation_stats[\"tools_used\"] += 1\n",
        "        tool_usage_history.append({\n",
        "            \"tool\": function_decision,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"result\": result,\n",
        "            \"question_number\": conversation_stats[\"questions_asked\"]\n",
        "        })\n",
        "\n",
        "        chat_history.append({\n",
        "            \"role\": \"function\",\n",
        "            \"name\": function_decision,\n",
        "            \"content\": result\n",
        "        })\n",
        "        chat_history.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": result\n",
        "        })\n",
        "\n",
        "    else:\n",
        "        # --- General LLM Response with Enhanced Memory ---\n",
        "        \n",
        "        # Create detailed memory context\n",
        "        memory_context = f\"\"\"\n",
        "Conversation Statistics:\n",
        "- Total questions asked: {conversation_stats[\"questions_asked\"]}\n",
        "- Total tools used: {conversation_stats[\"tools_used\"]}\n",
        "- Conversation started: {conversation_stats[\"start_time\"]}\n",
        "\n",
        "Tool Usage History:\n",
        "{chr(10).join([f\"- Question #{entry['question_number']}: Used {entry['tool']} ‚Üí {entry['result']} at {entry['timestamp']}\" for entry in tool_usage_history]) or \"No tools used yet.\"}\n",
        "\n",
        "Available Tools:\n",
        "1. calculate_sum - adds 7 + 5\n",
        "2. get_user_profile - gets profile for Bhavishya  \n",
        "3. give_motivational_quote - provides a motivational quote\n",
        "\"\"\"\n",
        "\n",
        "        full_prompt = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    f\"You are a helpful assistant with specific tools and conversation memory.\\n\"\n",
        "                    f\"{memory_context}\\n\\n\"\n",
        "                    f\"IMPORTANT RULES:\\n\"\n",
        "                    f\"1. You can ONLY perform actions using the three tools listed above\\n\"\n",
        "                    f\"2. You CAN answer questions about the conversation history, statistics, and available tools\\n\"\n",
        "                    f\"3. Be precise about what you can and cannot do\\n\"\n",
        "                    f\"4. Use the conversation statistics to answer questions accurately\\n\"\n",
        "                    f\"5. Never make up or hallucinate information\\n\"\n",
        "                    f\"Answer the user's current question based on the conversation history and available information.\"\n",
        "                )\n",
        "            }\n",
        "        ] + chat_history\n",
        "\n",
        "        general_response = groq_client.chat.completions.create(\n",
        "            model=DEFAULT_MODEL,\n",
        "            messages=full_prompt\n",
        "        )\n",
        "\n",
        "        output = general_response.choices[0].message.content\n",
        "        print(\"Assistant:\", output)\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": output})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qtHXKaeZmHIj",
      "metadata": {
        "id": "qtHXKaeZmHIj"
      },
      "source": [
        "## üìä Step 5: Evaluating the Assistant's Response\n",
        "\n",
        "This function will use another LLM call to assess the relevance and helpfulness of the assistant's response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "_Ld7wqSGmHIj",
      "metadata": {
        "id": "_Ld7wqSGmHIj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "`evaluate_response` function defined.\n",
            "LLM Response: ChatCompletion(id='chatcmpl-f28a7d07-dbe5-4b76-a441-a2355997ab6f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='There are several ways to reverse a string in Python:\\n\\n1. **Using slicing**: You can use slicing to reverse a string. The syntax is: `string[::-1]`. This will start from the end of the string and move backwards to the beginning, stepping backwards by 1 character each time.\\n\\nExample: `my_string = \"hello\"; reversed_string = my_string[::-1]; print(reversed_string)` Output: `\"olleh\"`\\n\\n2. **Using the `reversed` function**: The `reversed` function returns a reverse iterator. You can convert it to a string using the `join` method.\\n\\nExample: `my_string = \"hello\"; reversed_string = \"\".join(reversed(my_string)); print(reversed_string)` Output: `\"olleh\"`\\n\\n3. **Using the `reverse` method of a list**: You can convert the string to a list, reverse it using the `reverse` method, and then join it back into a string.\\n\\nExample: `my_string = \"hello\"; reversed_list = list(my_string)[::-1]; reversed_string = \"\".join(reversed_list); print(reversed_string)` Output: `\"olleh\"`\\n\\nChoose the method that best fits your needs!', role='assistant', executed_tools=None, function_call=None, reasoning=None, tool_calls=None))], created=1746942455, model='llama3-8b-8192', object='chat.completion', system_fingerprint='fp_dadc9d6142', usage=CompletionUsage(completion_tokens=244, prompt_tokens=19, total_tokens=263, completion_time=0.203333333, prompt_time=0.003024919, queue_time=0.24971702000000004, total_time=0.206358252), usage_breakdown=UsageBreakdown(models=None), x_groq={'id': 'req_01jtyy4v1ke5gb1qb0eq19h1vm'})\n",
            "Evaluation: Coding Relevance: Yes, Helpfulness: 5/5, Refusal Appropriateness: NA\n"
          ]
        }
      ],
      "source": [
        "def evaluate_response(client, user_query, assistant_response_content, model):\n",
        "    if not client:\n",
        "        print(\"Groq client is not initialized. Cannot evaluate response.\")\n",
        "        return \"Evaluation failed (client not initialized).\"\n",
        "\n",
        "    eval_system_prompt_content = f\"\"\"You are an evaluation AI. Evaluate the assistant's response based on the user's query.\n",
        "    User Query: \"{user_query}\"\n",
        "    Assistant Response: \"{assistant_response_content}\"\n",
        "\n",
        "    Evaluate based on these criteria:\n",
        "    1.  **Coding Relevance:** Was the assistant's response strictly related to coding/programming topics? (Yes/No)\n",
        "    2.  **Helpfulness (if relevant):** If the response was coding-related, how helpful and accurate was it? (Score 1-5, 5=Excellent, 1=Not Helpful, NA if not relevant)\n",
        "    3.  **Refusal Appropriateness (if irrelevant):** If the user's query was *not* coding-related, did the assistant politely refuse according to its instructions? (Yes/No/NA)\n",
        "\n",
        "    Provide the evaluation concisely, starting with \"Evaluation:\".\n",
        "    Example (Relevant): \"Evaluation: Coding Relevance: Yes, Helpfulness: 4/5, Refusal Appropriateness: NA\"\n",
        "    Example (Irrelevant, Correct Refusal): \"Evaluation: Coding Relevance: No, Helpfulness: NA, Refusal Appropriateness: Yes\"\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"system\", \"content\": eval_system_prompt_content}],\n",
        "        temperature=0.1,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "print(\"`evaluate_response` function defined.\")\n",
        "sample_user_query = \"How do I reverse a string in Python?\"\n",
        "\n",
        "sample_response = groq_client.chat.completions.create(\n",
        "        model=DEFAULT_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": sample_user_query}\n",
        "        ]\n",
        "    )\n",
        "print('LLM Response:', sample_response)\n",
        "print(evaluate_response(groq_client, sample_user_query, sample_response, DEFAULT_MODEL))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hupRDqXamHIj",
      "metadata": {
        "id": "hupRDqXamHIj"
      },
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You've successfully set up and interacted with a Groq-powered Coding Assistant!\n",
        "\n",
        "**Next Steps & Ideas:**\n",
        "- Try different coding questions.\n",
        "- Ask a non-coding question to see the refusal.\n",
        "- Experiment with models and tools."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
