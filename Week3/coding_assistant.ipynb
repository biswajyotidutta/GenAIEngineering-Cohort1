{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "V3SNDff3mHIe",
      "metadata": {
        "id": "V3SNDff3mHIe"
      },
      "source": [
        " ü§ñ My First Groq-Powered Coding Assistant (Notebook Edition)\n",
        "\n",
        "Welcome! This Jupyter Notebook will guide you through creating a simple coding assistant that uses the Groq API for fast responses from Large Language Models (LLMs) like Llama 3.\n",
        "\n",
        "**What you'll learn:**\n",
        "- How to install and import necessary Python libraries.\n",
        "- How to securely configure your Groq API key.\n",
        "- How to connect to the Groq service.\n",
        "- How to send prompts to an LLM and get responses.\n",
        "- How to use \"tools\" (like getting the current time) with the LLM.\n",
        "- How to evaluate the LLM's responses.\n",
        "- How to create a basic interactive chat loop.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yDlnj02pmHIg",
      "metadata": {
        "id": "yDlnj02pmHIg"
      },
      "source": [
        "## ‚öôÔ∏è Step 0: Setup - Install Libraries\n",
        "\n",
        "First, we need to make sure we have the `groq` library (for interacting with the Groq API) and `python-dotenv` (for managing our API key securely).\n",
        "\n",
        "**Instructions:**\n",
        "1. Run the code cell below **once**.\n",
        "2. If the libraries are already installed, it won't do any harm.\n",
        "3. After running it successfully, you can comment out the `!pip install` lines (by adding a `#` at the beginning of those lines) to avoid running them every time you open the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gRjSm2YLmHIg",
      "metadata": {
        "id": "gRjSm2YLmHIg"
      },
      "outputs": [],
      "source": [
        "# Before running this, make sure you have Python installed.\n",
        "# You can run these commands in your terminal or directly in a notebook cell by adding \"!\" at the beginning.\n",
        "# If you run them here, they only need to be run once.\n",
        "\n",
        "# print(\"Installing necessary libraries...\")\n",
        "# !pip install groq python-dotenv\n",
        "\n",
        "# print(\"Installation complete! You can now comment out the !pip install lines or remove this cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f8498ad",
      "metadata": {},
      "source": [
        "### Understanding F Strings and Format Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4a5d5719",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, Alice!\n",
            "Hello, Bhavishya\n",
            "I am learning f-string and format function today!\n",
            "Course name: DATA SCIENCE\n",
            "Employee Dana worked on Analysis for 4 hours.\n"
          ]
        }
      ],
      "source": [
        "# Let us understand what is f string and .format() function as well. \n",
        "\n",
        "# Simple Example\n",
        "name = \"Alice\"\n",
        "greeting = f\"Hello, {name}!\"\n",
        "print(greeting)\n",
        "\n",
        "greeting = 'Hello, {name}'\n",
        "print(greeting.format(name='Bhavishya'))\n",
        "\n",
        "message = 'I am learning {} and {} today!'\n",
        "print(message.format('f-string', 'format function'))\n",
        "\n",
        "# f string with function call\n",
        "def convert_to_upper(text):\n",
        "    return text.upper()\n",
        "\n",
        "name = \"data science\"\n",
        "print(f\"Course name: {convert_to_upper(name)}\")\n",
        "\n",
        "\n",
        "# f strings with dictionary\n",
        "data = {\"name\": \"Dana\", \"task\": \"Analysis\", \"hours\": 4}\n",
        "template = \"Employee {name} worked on {task} for {hours} hours.\"\n",
        "print(template.format(**data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e0349e",
      "metadata": {},
      "source": [
        "### Hands-on Activity - Format Function and F-String"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdbc6281",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask the user for a number using input(). Then use an f-string and format function to print the number squared."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f36f3fdf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write a function greet(name, role) that returns a greeting using f-strings and format():\n",
        "# greet(\"Tina\", \"Manager\") ‚ûû \"Hello Tina! You are working as a Manager.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9PUX1XFwmHIh",
      "metadata": {
        "id": "9PUX1XFwmHIh"
      },
      "source": [
        "Now, let's import the libraries we'll need for this project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Lal5NYowmHIh",
      "metadata": {
        "id": "Lal5NYowmHIh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from groq import Groq, RateLimitError, APIError\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv # For loading API key from a .env file\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4hOj3rQAmHIh",
      "metadata": {
        "id": "4hOj3rQAmHIh"
      },
      "source": [
        "## üîë Step 1: Configure Your Groq API Key\n",
        "\n",
        "To use the Groq API, you need an API key. You can get one from the [Groq Console](https://console.groq.com/keys).\n",
        "\n",
        "\n",
        "**For Local Jupyter:**\n",
        "1.  **Create a file named `.env`** in the *same directory* as this notebook.\n",
        "2.  Open the `.env` file with a text editor.\n",
        "3.  Add the following line, replacing `your_gsk_key_here` with your actual Groq API key:\n",
        "    ```\n",
        "    GROQ_API_KEY=\"your_gsk_key_here\"\n",
        "    ```\n",
        "4.  Save the `.env` file.\n",
        "\n",
        "**For Google Colab (Preferred for Colab):**\n",
        "1. Click the **key icon (üîë)** in the left sidebar of Colab.\n",
        "2. Click `+ ADD A NEW SECRET`.\n",
        "3. Name: `GROQ_API_KEY`\n",
        "4. Value: Your actual Groq API key.\n",
        "5. Ensure 'Notebook access' is ON.\n",
        "The code in the next cell is set up for local `.env` but will need adjustment for Colab secrets (see Colab instructions I provided earlier)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "XvyfmGa_mHIh",
      "metadata": {
        "id": "XvyfmGa_mHIh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq API Key loaded (or attempted from environment/Colab Secrets).\n",
            "Using model: llama3-8b-8192\n",
            "Using evaluation model: llama3-8b-8192\n"
          ]
        }
      ],
      "source": [
        "# Load environment variables from .env file (for local use)\n",
        "# For Colab, you'd use: from google.colab import userdata; GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "load_dotenv()\n",
        "\n",
        "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
        "\n",
        "if not GROQ_API_KEY:\n",
        "    print(\"GROQ_API_KEY not found. Ensure it's in .env (local) or Colab Secrets and you've adapted this cell if in Colab.\")\n",
        "    # Fallback for manual input if needed, but not recommended for routine use:\n",
        "    # GROQ_API_KEY = input(\"Please enter your Groq API Key: \")\n",
        "    if not GROQ_API_KEY: # Check again if it was entered\n",
        "        raise ValueError(\n",
        "            \"Groq API Key is not set. Please configure it and restart the kernel.\"\n",
        "        )\n",
        "else:\n",
        "    print(\"Groq API Key loaded (or attempted from environment/Colab Secrets).\")\n",
        "\n",
        "# --- Configuration ---\n",
        "DEFAULT_MODEL = \"llama3-8b-8192\"\n",
        "EVALUATION_MODEL = \"llama3-8b-8192\" # Can be the same or different\n",
        "\n",
        "print(f\"Using model: {DEFAULT_MODEL}\")\n",
        "print(f\"Using evaluation model: {EVALUATION_MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5_R1UWXamHIi",
      "metadata": {
        "id": "5_R1UWXamHIi"
      },
      "source": [
        "## üîó Step 2: Connect to the Groq Client\n",
        "\n",
        "Now that we have the API key, let's write a function to initialize the Groq client. This client is what we'll use to communicate with the Groq API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "8-QsyNtCmHIi",
      "metadata": {
        "id": "8-QsyNtCmHIi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq client initialized successfully.\n",
            "LLM Test Call Response: Hello! It's nice to meet you! I'm here to help with any questions or tasks you may have. What can I assist you with today?\n",
            "Successfully connected to Groq!\n"
          ]
        }
      ],
      "source": [
        "def get_groq_client():\n",
        "    \"\"\"Initializes and returns the Groq client.\"\"\"\n",
        "    \n",
        "    try:\n",
        "        client = Groq(api_key=GROQ_API_KEY)\n",
        "        print(\"Groq client initialized successfully.\")\n",
        "\n",
        "        # --- LLM call demo ---\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=DEFAULT_MODEL,\n",
        "                messages=[{\"role\": \"user\", \"content\": \"Say hello! This is a test LLM call from get_groq_client.\"}],\n",
        "                temperature=0.5,\n",
        "                max_tokens=200\n",
        "            )\n",
        "            print(\"LLM Test Call Response:\", response.choices[0].message.content)\n",
        "        except Exception as llm_error:\n",
        "            print(\"LLM call failed:\", llm_error)\n",
        "        # --- End demo ---\n",
        "\n",
        "        return client\n",
        "    except APIError as e:\n",
        "        print(f\"Failed to initialize Groq client due to APIError: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during client initialization: {e}\")\n",
        "        return None\n",
        "\n",
        "# Attempt to initialize the client\n",
        "groq_client = get_groq_client()\n",
        "\n",
        "if groq_client:\n",
        "    print(\"Successfully connected to Groq!\")\n",
        "else:\n",
        "    print(\"Failed to connect to Groq. Please check your API key and network connection.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qoIDLZ-YmHIi",
      "metadata": {
        "id": "qoIDLZ-YmHIi"
      },
      "source": [
        "## üõ†Ô∏è Step 3: Define Helper Functions\n",
        "\n",
        "We need a few helper functions:\n",
        "1.  `get_current_datetime`: An example function that the LLM can \"call\" if it needs the current time to answer a coding question.\n",
        "2.  `filter_messages_for_api`: This function ensures we only send data to the API that it expects, removing any custom keys we might add for our own use (like evaluation results)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "cD8culvOmHIi",
      "metadata": {
        "id": "cD8culvOmHIi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helper functions defined.\n",
            "Groq client initialized successfully.\n",
            "LLM Test Call Response: Hello! It's great to hear from you! I'm here to assist with any questions or tasks you may have.\n",
            "Current datetime JSON: {\"current_datetime\": \"2025-05-10T21:17:10.329672\"}\n",
            "Filtered messages for LLM API: [{'role': 'user', 'content': 'Can you greet the user and also acknowledge the current time: {\"current_datetime\": \"2025-05-10T21:17:10.329672\"}?'}]\n",
            "LLM Response: What a lovely request!\n",
            "\n",
            "Here's a warm greeting and a nod to the current time:\n",
            "\n",
            "**Hello! It's great to chat with you! **\n",
            "\n",
            "As I check the current time, I see that it's currently **May 10th, 2025 at 9:17 PM**.\n"
          ]
        }
      ],
      "source": [
        "def get_current_datetime():\n",
        "    \"\"\"Returns the current date and time as a JSON string.\"\"\"\n",
        "    return json.dumps({\"current_datetime\": datetime.now().isoformat()})\n",
        "\n",
        "def filter_messages_for_api(messages):\n",
        "    api_messages = []\n",
        "    for msg in messages:\n",
        "        api_msg = msg.copy()\n",
        "        api_msg.pop(\"evaluation\", None)\n",
        "        api_messages.append(api_msg)\n",
        "    return api_messages\n",
        "\n",
        "print(\"Helper functions defined.\")\n",
        "\n",
        "# --- Demo: Use the helper functions with a real LLM call ---\n",
        "groq_client = get_groq_client()  # Reuse the enhanced client setup\n",
        "\n",
        "if groq_client:\n",
        "    # Get current datetime\n",
        "    datetime_json = get_current_datetime()\n",
        "    print(\"Current datetime JSON:\", datetime_json)\n",
        "\n",
        "    # Construct message with flavor\n",
        "    raw_messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Can you greet the user and also acknowledge the current time: {datetime_json}?\",\n",
        "            \"evaluation\": \"demo\"  # This key should be stripped\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    filtered_messages = filter_messages_for_api(raw_messages)\n",
        "    print(\"Filtered messages for LLM API:\", filtered_messages)\n",
        "\n",
        "    try:\n",
        "        response = groq_client.chat.completions.create(\n",
        "            model=DEFAULT_MODEL,\n",
        "            messages=filtered_messages\n",
        "        )\n",
        "        print(\"LLM Response:\", response.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        print(\"LLM call failed:\", e)\n",
        "else:\n",
        "    print(\"Skipping LLM call demo due to client initialization failure.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d08222f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq client initialized successfully.\n",
            "LLM Test Call Response: Hello! It's great to hear from you! I'm here to help with any questions or topics you'd like to discuss. It looks like this is a test call from get_groq_client, is that correct? If you're looking to test my language understanding or just want to chat, I'm happy to oblige. What's on your mind?\n",
            "LLM chose to call: get_current_datetime\n",
            "Function 'get_current_datetime' result: {\"current_datetime\": \"2025-05-10T21:17:27.122378\"}\n",
            "LLM Final Explanation: The result of `get_current_datetime` is a Python dictionary that contains a single key-value pair. The key is `\"current_datetime\"`, and the value is a string that represents the current date and time.\n",
            "\n",
            "In this particular case, the value is the string `\"2025-05-10T21:17:27.122378\"`.\n",
            "\n",
            "Let's break it down:\n",
            "\n",
            "* `2025-05-10`: This is the date in the format \"Year-Month-Day\".\n",
            "* `T`: This is a literal character that separates the date from the time.\n",
            "* `21:17:27`: This is the time in the format \"Hour:Minute:Second\".\n",
            "* `.122378`: This is the precision of the timestamp, representing the microseconds (1/1,000,000th of a second) that have passed since the Unix epoch (January 1, 1970, 00:00:00 UTC).\n",
            "\n",
            "So, if I were to explain it to the user, I would say:\n",
            "\n",
            "\"Hello! The result of `get_current_datetime` is a timestamp that represents the current date and time. It's in the format `Year-Month-DayTHour:Minute:Second_precision`, where the precision is the number of microseconds that have passed since the Unix epoch. In this case, the current date and time is `2025-05-10T21:17:27.122378`, which means it's currently `21:17:27` on May 10, 2025, with 122,378 microseconds having passed since the start of the day.\"\n"
          ]
        }
      ],
      "source": [
        "# Tool function 1\n",
        "def get_current_datetime():\n",
        "    \"\"\"Returns the current date and time as a JSON string.\"\"\"\n",
        "    return json.dumps({\"current_datetime\": datetime.now().isoformat()})\n",
        "\n",
        "# Tool function 2 (a dummy example)\n",
        "def get_welcome_message(name=\"User\"):\n",
        "    \"\"\"Returns a friendly welcome message.\"\"\"\n",
        "    return f\"Hello, {name}! Welcome to your AI assistant demo.\"\n",
        "\n",
        "# Simulated tool registry\n",
        "tool_functions = {\n",
        "    \"get_current_datetime\": get_current_datetime,\n",
        "    \"get_welcome_message\": lambda: get_welcome_message(\"Bhavishya\")\n",
        "}\n",
        "\n",
        "# Simulated question that will trigger function call logic\n",
        "user_question = \"Can you tell me the current time?\"\n",
        "\n",
        "# Get Groq client\n",
        "groq_client = get_groq_client()\n",
        "\n",
        "if groq_client:\n",
        "    # Send the user question to Groq to determine which function to call\n",
        "    routing_response = groq_client.chat.completions.create(\n",
        "        model=DEFAULT_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that decides which function to call. Only return the name of the function nothing else.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Question: {user_question}\\n\"\n",
        "                                        f\"Available functions: get_current_datetime, get_welcome_message\\n\"\n",
        "                                        f\"Reply ONLY with the function name to call (no extra words).\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    chosen_function = routing_response.choices[0].message.content.strip()\n",
        "    print(f\"LLM chose to call: {chosen_function}\")\n",
        "\n",
        "    # Call the corresponding function if valid\n",
        "    if chosen_function in tool_functions:\n",
        "        result = tool_functions[chosen_function]()\n",
        "        print(f\"Function '{chosen_function}' result:\", result)\n",
        "\n",
        "        # Send the result back to the LLM for response generation\n",
        "        final_response = groq_client.chat.completions.create(\n",
        "            model=DEFAULT_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": f\"The result of {chosen_function} is: {result}. \"\n",
        "                                            f\"Can you explain this nicely to the user?\"}\n",
        "            ]\n",
        "        )\n",
        "        print(\"LLM Final Explanation:\", final_response.choices[0].message.content)\n",
        "    else:\n",
        "        print(\"Invalid function name returned by LLM.\")\n",
        "else:\n",
        "    print(\"Groq client initialization failed. Cannot demonstrate function calling.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbGART19mHIi",
      "metadata": {
        "id": "dbGART19mHIi"
      },
      "source": [
        "## üí¨ Step 4: Adding Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "Jh3P0tC_mHIi",
      "metadata": {
        "id": "Jh3P0tC_mHIi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq client initialized successfully.\n",
            "LLM Test Call Response: Hello! It's great to hear from you! I'm here to help with any questions or tasks you may have.\n",
            "LLM chose to call: get_current_datetime\n",
            "Function 'get_current_datetime' result: {\"current_datetime\": \"2025-05-10T21:17:41.665088\"}\n",
            "LLM Final Explanation: I see what you did there!\n",
            "\n",
            "I'm a large language model, I don't have direct access to the current time, but I can try to help you with that.\n",
            "\n",
            "When you called the `get_current_datetime` function, it returned the current date and time in a JSON format, which looks like this: `{\"current_datetime\": \"2025-05-10T21:17:41.665088\"}`.\n",
            "\n",
            "This format is a standard way of representing dates and times in computers. Let me break it down for you:\n",
            "\n",
            "* The `\"current_datetime\"` part is the label for the current date and time.\n",
            "* The value that follows, `\"2025-05-10T21:17:41.665088\"`, is the actual date and time.\n",
            "\n",
            "Here's what each part of the value represents:\n",
            "\t+ `\"2025-05-10\"` is the date in the format `YYYY-MM-DD`.\n",
            "\t+ `\"T21:17:41\"` is the time in the format `HH:MM:SS`, where `HH` is the hour (in 24-hour format), `MM` is the minute, and `SS` is the second.\n",
            "\t+ `.665088` is the fractional part of the second, which is a small fraction of a second.\n",
            "\n",
            "So, putting it all together, the returned value indicates that the current date and time is May 10th, 2025, at 9:17:41 PM (or 21:17:41 in 24-hour format).\n",
            "\n",
            "I hope that helps!\n"
          ]
        }
      ],
      "source": [
        "# Memory: store all conversation turns here\n",
        "chat_history = []\n",
        "\n",
        "# Simulated incoming question from user\n",
        "user_question = \"Can you tell me the current time?\"\n",
        "\n",
        "# Get Groq client\n",
        "groq_client = get_groq_client()\n",
        "\n",
        "if groq_client:\n",
        "    # Append user question to history\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_question})\n",
        "\n",
        "    # System prompt to instruct the LLM\n",
        "    system_prompt = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"You are a helpful assistant that decides which function to call. \"\n",
        "            \"Reply ONLY with the function name (e.g., get_current_datetime), nothing else.\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Add the routing query to chat\n",
        "    routing_messages = chat_history.copy()\n",
        "    routing_messages.insert(0, system_prompt)\n",
        "    routing_messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Available functions: {', '.join(tool_functions.keys())}\\n\"\n",
        "                   f\"Decide which one to call based on the last user input.\"\n",
        "    })\n",
        "\n",
        "    # Ask LLM to route the function\n",
        "    routing_response = groq_client.chat.completions.create(\n",
        "        model=DEFAULT_MODEL,\n",
        "        messages=routing_messages\n",
        "    )\n",
        "\n",
        "    chosen_function = routing_response.choices[0].message.content.strip()\n",
        "    print(f\"LLM chose to call: {chosen_function}\")\n",
        "\n",
        "    # Save assistant response (function name)\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": chosen_function})\n",
        "\n",
        "    if chosen_function in tool_functions:\n",
        "        result = tool_functions[chosen_function]()\n",
        "        print(f\"Function '{chosen_function}' result:\", result)\n",
        "\n",
        "        # Add the function result as a message to the history\n",
        "        chat_history.append({\n",
        "            \"role\": \"function\",\n",
        "            \"name\": chosen_function,\n",
        "            \"content\": result\n",
        "        })\n",
        "\n",
        "        # Now let LLM summarize/explain this in a friendly way\n",
        "        final_response = groq_client.chat.completions.create(\n",
        "            model=DEFAULT_MODEL,\n",
        "            messages=chat_history + [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"The result of {chosen_function} was returned. Please explain that to the user nicely.\"\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        friendly_output = final_response.choices[0].message.content\n",
        "        print(\"LLM Final Explanation:\", friendly_output)\n",
        "\n",
        "        # Save final assistant message\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": friendly_output})\n",
        "    else:\n",
        "        print(\"Invalid function name returned by LLM.\")\n",
        "else:\n",
        "    print(\"Groq client initialization failed. Cannot demonstrate function calling.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3df6f038",
      "metadata": {},
      "source": [
        "### Stateful AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a8fd0e0c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groq client initialized successfully.\n",
            "LLM Test Call Response: Hello! It's great to hear from you. I'm here to help with any questions or tasks you may have. Please feel free to ask me anything.\n",
            "[Tool: calculate_sum] The sum of 7 and 5 is 12.\n",
            "Assistant: , you called `calculate_sum` with arguments 7 and 5 once\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# --- Tool Functions ---\n",
        "def calculate_sum(a, b):\n",
        "    return f\"The sum of {a} and {b} is {a + b}.\"\n",
        "\n",
        "def get_user_profile(name=\"Guest\"):\n",
        "    return f\"{name} is a curious learner exploring AI tools!\"\n",
        "\n",
        "def give_motivational_quote():\n",
        "    return \"Believe in yourself. Every expert was once a beginner.\"\n",
        "\n",
        "# --- Tool Registry ---\n",
        "tool_functions = {\n",
        "    \"calculate_sum\": lambda: calculate_sum(7, 5),\n",
        "    \"get_user_profile\": lambda: get_user_profile(\"Bhavishya\"),\n",
        "    \"give_motivational_quote\": give_motivational_quote\n",
        "}\n",
        "\n",
        "# --- Memory ---\n",
        "chat_history = []\n",
        "tool_usage_history = []\n",
        "\n",
        "# --- Groq Client ---\n",
        "groq_client = get_groq_client()\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\", \"end\"]:\n",
        "        break\n",
        "\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # --- Tool Routing Phase ---\n",
        "    routing_prompt = [\n",
        "        {\"role\": \"system\", \"content\": (\n",
        "            \"You are a smart assistant with tools: calculate_sum, get_user_profile, give_motivational_quote.\\n\"\n",
        "            \"If the user input requires one of these tools, respond ONLY with the tool name.\\n\"\n",
        "            \"If it doesn't, respond with 'none'.\"\n",
        "        )}\n",
        "    ] + chat_history[-3:] + [\n",
        "        {\"role\": \"user\", \"content\": \"Which tool should be used now? Or reply 'none'.\"}\n",
        "    ]\n",
        "\n",
        "    routing_response = groq_client.chat.completions.create(\n",
        "        model=DEFAULT_MODEL,\n",
        "        messages=routing_prompt\n",
        "    )\n",
        "\n",
        "    function_decision = routing_response.choices[0].message.content.strip()\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": function_decision})\n",
        "\n",
        "    if function_decision in tool_functions:\n",
        "        result = tool_functions[function_decision]()\n",
        "        print(f\"[Tool: {function_decision}] {result}\")\n",
        "\n",
        "        tool_usage_history.append({\n",
        "            \"tool\": function_decision,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"result\": result\n",
        "        })\n",
        "\n",
        "        chat_history.append({\n",
        "            \"role\": \"function\",\n",
        "            \"name\": function_decision,\n",
        "            \"content\": result\n",
        "        })\n",
        "\n",
        "    else:\n",
        "        # --- General LLM Response with Memory Awareness ---\n",
        "\n",
        "        # üí° Fix: move memory to TOP system message\n",
        "        memory_context = \"\\n\".join([\n",
        "            f\"- {entry['tool']} ‚Üí {entry['result']} at {entry['timestamp']}\"\n",
        "            for entry in tool_usage_history\n",
        "        ]) or \"No tools used yet.\"\n",
        "\n",
        "        full_prompt = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    f\"You are a helpful assistant. Here is the user's tool usage history:\\n\"\n",
        "                    f\"{memory_context}\\n\"\n",
        "                    f\"Answer based on current message and history.\"\n",
        "                )\n",
        "            }\n",
        "        ] + chat_history\n",
        "\n",
        "        general_response = groq_client.chat.completions.create(\n",
        "            model=DEFAULT_MODEL,\n",
        "            messages=full_prompt\n",
        "        )\n",
        "\n",
        "        output = general_response.choices[0].message.content\n",
        "        print(\"Assistant:\", output)\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": output})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qtHXKaeZmHIj",
      "metadata": {
        "id": "qtHXKaeZmHIj"
      },
      "source": [
        "## üìä Step 5: Evaluating the Assistant's Response\n",
        "\n",
        "This function will use another LLM call to assess the relevance and helpfulness of the assistant's response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_Ld7wqSGmHIj",
      "metadata": {
        "id": "_Ld7wqSGmHIj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "`evaluate_response` function defined.\n",
            "Evaluation: Coding Relevance: Yes, Helpfulness: 5/5, Refusal Appropriateness: NA\n"
          ]
        }
      ],
      "source": [
        "def evaluate_response(client, user_query, assistant_response_content, model):\n",
        "    if not client:\n",
        "        print(\"Groq client is not initialized. Cannot evaluate response.\")\n",
        "        return \"Evaluation failed (client not initialized).\"\n",
        "\n",
        "    eval_system_prompt_content = f\"\"\"You are an evaluation AI. Evaluate the assistant's response based on the user's query.\n",
        "    User Query: \"{user_query}\"\n",
        "    Assistant Response: \"{assistant_response_content}\"\n",
        "\n",
        "    Evaluate based on these criteria:\n",
        "    1.  **Coding Relevance:** Was the assistant's response strictly related to coding/programming topics? (Yes/No)\n",
        "    2.  **Helpfulness (if relevant):** If the response was coding-related, how helpful and accurate was it? (Score 1-5, 5=Excellent, 1=Not Helpful, NA if not relevant)\n",
        "    3.  **Refusal Appropriateness (if irrelevant):** If the user's query was *not* coding-related, did the assistant politely refuse according to its instructions? (Yes/No/NA)\n",
        "\n",
        "    Provide the evaluation concisely, starting with \"Evaluation:\".\n",
        "    Example (Relevant): \"Evaluation: Coding Relevance: Yes, Helpfulness: 4/5, Refusal Appropriateness: NA\"\n",
        "    Example (Irrelevant, Correct Refusal): \"Evaluation: Coding Relevance: No, Helpfulness: NA, Refusal Appropriateness: Yes\"\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"system\", \"content\": eval_system_prompt_content}],\n",
        "        temperature=0.1,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "print(\"`evaluate_response` function defined.\")\n",
        "sample_user_query = \"How do I reverse a string in Python?\"\n",
        "\n",
        "sample_response = groq_client.chat.completions.create(\n",
        "        model=DEFAULT_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": sample_user_query}\n",
        "        ]\n",
        "    )\n",
        "print('LLM Response:', sample_response)\n",
        "print(evaluate_response(groq_client, sample_user_query, sample_response, DEFAULT_MODEL))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hupRDqXamHIj",
      "metadata": {
        "id": "hupRDqXamHIj"
      },
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You've successfully set up and interacted with a Groq-powered Coding Assistant!\n",
        "\n",
        "**Next Steps & Ideas:**\n",
        "- Try different coding questions.\n",
        "- Ask a non-coding question to see the refusal.\n",
        "- Experiment with models and tools."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
